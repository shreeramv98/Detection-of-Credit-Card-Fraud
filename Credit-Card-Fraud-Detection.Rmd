---
title: "Credit Card Fraud Detection"
author: "Shreeram Venkatesh"
date: "2023-10-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PROJECT CODE :



```{r}
library(ranger)  
library(caret)  
library(data.table)  
library(pROC)
library(caTools)  
credcarddataset <- read.csv("D:/SourceCodeforMSProjects/R-Project/Credit Card Fraud Detection/creditcard.csv")

dim(credcarddataset)
head(credcarddataset,6)
tail(credcarddataset,6)
table(credcarddataset$Class)
summary(credcarddataset$Amount)
names(credcarddataset)
var(credcarddataset$Amount)
sd(credcarddataset$Amount)
head(credcarddataset)

# Assuming 'credcarddataset' is your dataset

credcarddataset$Amount <- scale(credcarddataset$Amount)
latestdata=credcarddataset[,-c(1)]
head(latestdata)
set.seed(123)
sample_dataset = sample.split(latestdata$Class,SplitRatio=0.80)
trainingdata = subset(latestdata,sample_dataset==TRUE)
testingdata = subset(latestdata,sample_dataset==FALSE)
dim(trainingdata) 
dim(testingdata)
logist_model = glm(Class~.,testingdata,family=binomial())
summary(logist_model)                     
plot(logist_model)

logist_model = glm(Class~.,trainingdata,family=binomial())
summary(logist_model)
# Show and examine ROC curves
library(pROC)
lr.predict <- predict(logist_model,testingdata, probability = TRUE)
auc.gbm = roc(testingdata$Class, lr.predict, plot = TRUE, col = "green")
# Recursive partitioning for classification, regression and survival trees.
library(rpart)
library(rpart.plot)
modeldectree <- rpart(Class ~ . , credcarddataset, method = 'class')
forcastedvalue <- predict(modeldectree, credcarddataset, type = 'class')
probability <- predict(modeldectree, credcarddataset, type = 'prob')

rpart.plot(modeldectree)
# nn for calculating a specified neural network for specified covariate vectors
library(neuralnet)
modelANN <- neuralnet(Class~.,data=trainingdata,linear.output=F)
plot(modelANN)

ANN_prediction=compute(modelANN,testingdata)
outputANN=ANN_prediction$net.result
outputANN=ifelse(outputANN>0.5,1,0)
# An implementation of gradient boosting machine proposed by Friedman and modifications to the AdaBoost method by Freund and Schapire.
library(gbm, quietly=TRUE)
# Spend enough time training the GBM model.
system.time(
  model_gbm <- gbm(Class ~ .
                   , distribution = "bernoulli"
                   , data = rbind(trainingdata, testingdata)
                   , n.trees = 500
                   , interaction.depth = 3
                   , n.minobsinnode = 100
                   , shrinkage = 0.01
                   , bag.fraction = 0.5
                   , train.fraction = nrow(trainingdata) / (nrow(trainingdata) + nrow(testingdata))
  )
)

# Determine best iteration based on test data
gbm.iter = gbm.perf(model_gbm, method = "test")
model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE)

plot(model_gbm)
gbm_test = predict(model_gbm, newdata = testingdata, n.trees = gbm.iter)
gbm_auc = roc(testingdata$Class, gbm_test, plot = TRUE, col = "red")
print(gbm_auc)
                     
                     

```

